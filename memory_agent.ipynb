{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "chat_message_history = SQLChatMessageHistory(\n",
    "    session_id=\"test_session_id\", connection=\"sqlite:///sqlite.db\"\n",
    ")\n",
    "\n",
    "chat_message_history.add_user_message(\"Hello\")\n",
    "chat_message_history.add_ai_message(\"Hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello'),\n",
       " AIMessage(content='Hi'),\n",
       " HumanMessage(content='Hello'),\n",
       " AIMessage(content='Hi')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_message_history.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | ChatOpenAI()\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: SQLChatMessageHistory(\n",
    "        session_id=session_id, connection=\"sqlite:///sqlite.db\"\n",
    "    ),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 13c79427-d6c4-4a24-a00e-df6a48966c5b not found for run 7f456764-b535-49cc-88ad-7d3658aec0fb. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob. How can I assist you, Bob?', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 102, 'total_tokens': 115}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d8b1a216-e451-4e93-87e9-39716f7bc1e6-0', usage_metadata={'input_tokens': 102, 'output_tokens': 13, 'total_tokens': 115})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"<SQL_SESSION_ID>\"}}\n",
    "\n",
    "chain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 3bf189ff-0293-4958-8098-462fb6c7ac34 not found for run d5927bc0-b75f-47bf-b549-f53f3e51a11d. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bob! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 56, 'total_tokens': 66}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-59451bcc-b8ca-4bc9-aa70-37cace63979c-0', usage_metadata={'input_tokens': 56, 'output_tokens': 10, 'total_tokens': 66})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is where we configure the session id\n",
    "\n",
    "chain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query category: personal\n",
      "Handling personal query...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize Pinecone and OctoAI clients\n",
    "octoai_api_token = os.environ.get(\"OCTOAI_API_TOKEN\")\n",
    "pc_key = os.environ.get(\"pc_key\")\n",
    "\n",
    "# Load Atomic Habits keywords from the provided file\n",
    "file_path = '/Users/clydeclarke/Documents/AH_router/data/ah_index.csv'\n",
    "csv_path = \"/Users/clydeclarke/Documents/AH_Code_Architecture/data/ah_index.csv\"\n",
    "index_data = pd.read_csv(csv_path)\n",
    "index_data.columns = index_data.columns.str.strip()\n",
    "\n",
    "# Extract concepts and words related to \"Atomic Habits\"\n",
    "atomic_habits_keywords = index_data['concept'].tolist()\n",
    "\n",
    "# Pre-defined personal keywords\n",
    "personal_keywords = [\"my\", \"I\", \"me\", \"mine\", \"family\", \"work\", \"personal\", \"health\"]\n",
    "\n",
    "# Load pre-trained model for embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large\")\n",
    "model = AutoModel.from_pretrained(\"thenlper/gte-large\")\n",
    "\n",
    "# Function to classify query using keywords\n",
    "def classify_query_keyword(query):\n",
    "    if any(keyword in query for keyword in personal_keywords):\n",
    "        return \"personal\"\n",
    "    if any(keyword in query for keyword in atomic_habits_keywords):\n",
    "        return \"atomic_habits\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Function to get embedding for a query\n",
    "def get_embedding(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Function to classify query using embeddings\n",
    "def classify_query_embedding(query):\n",
    "    query_embedding = get_embedding(query)\n",
    "    personal_embedding = get_embedding(\"This is a personal query.\")\n",
    "    atomic_habits_embedding = get_embedding(\"This is a query about the book Atomic Habits.\")\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity\n",
    "    personal_similarity = cosine_similarity(query_embedding, personal_embedding)\n",
    "    atomic_habits_similarity = cosine_similarity(query_embedding, atomic_habits_embedding)\n",
    "\n",
    "    if personal_similarity > atomic_habits_similarity:\n",
    "        return \"personal\"\n",
    "    else:\n",
    "        return \"atomic_habits\"\n",
    "\n",
    "# Combined function to classify query\n",
    "def classify_query(query):\n",
    "    category = classify_query_keyword(query)\n",
    "    if category == \"unknown\":\n",
    "        category = classify_query_embedding(query)\n",
    "    return category\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your query: \")\n",
    "category = classify_query(query)\n",
    "print(f\"Query category: {category}\")\n",
    "\n",
    "# Further actions based on the classification\n",
    "if category == \"personal\":\n",
    "    print(\"Handling personal query...\")\n",
    "    # Handle personal query\n",
    "elif category == \"atomic_habits\":\n",
    "    print(\"Handling Atomic Habits query...\")\n",
    "    # Handle Atomic Habits query\n",
    "else:\n",
    "    print(\"Unknown category. Please clarify your query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'personal'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize Pinecone and OctoAI clients\n",
    "octoai_api_token = os.environ.get(\"OCTOAI_API_TOKEN\")\n",
    "pc_key = os.environ.get(\"pc_key\")\n",
    "\n",
    "# Load keywords from CSV file\n",
    "def load_keywords_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    keywords = df['keywords'].tolist()  # Assuming the CSV has a column named 'keywords'\n",
    "    return keywords\n",
    "\n",
    "# Load Atomic Habits keywords from the provided file\n",
    "file_path = '/mnt/data/ah_index.csv'\n",
    "atomic_habits_keywords = load_keywords_from_csv(file_path)\n",
    "\n",
    "# Pre-defined personal keywords\n",
    "personal_keywords = [\"my\", \"I\", \"me\", \"mine\", \"family\", \"work\", \"personal\", \"health\"]\n",
    "\n",
    "# Load pre-trained model for embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large\")\n",
    "model = AutoModel.from_pretrained(\"thenlper/gte-large\")\n",
    "\n",
    "# Function to classify query using enhanced keywords\n",
    "def classify_query_keyword(query):\n",
    "    # Check if query contains any atomic habits keywords\n",
    "    for keyword in atomic_habits_keywords:\n",
    "        if keyword.lower() in query.lower():\n",
    "            return \"atomic_habits\"\n",
    "    # Check if query contains any personal keywords\n",
    "    for keyword in personal_keywords:\n",
    "        if keyword.lower() in query.lower():\n",
    "            return \"personal\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Function to get embedding for a query\n",
    "def get_embedding(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Function to classify query using embeddings\n",
    "def classify_query_embedding(query):\n",
    "    query_embedding = get_embedding(query)\n",
    "    personal_embedding = get_embedding(\"This is a personal query.\")\n",
    "    atomic_habits_embedding = get_embedding(\"This is a query about the book Atomic Habits.\")\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity\n",
    "    personal_similarity = cosine_similarity(query_embedding, personal_embedding, dim=1)\n",
    "    atomic_habits_similarity = cosine_similarity(query_embedding, atomic_habits_embedding, dim=1)\n",
    "\n",
    "    if personal_similarity.item() > atomic_habits_similarity.item():\n",
    "        return \"personal\"\n",
    "    else:\n",
    "        return \"atomic_habits\"\n",
    "\n",
    "# Combined function to classify query\n",
    "def classify_query(query):\n",
    "    category = classify_query_keyword(query)\n",
    "    if category == \"unknown\":\n",
    "        category = classify_query_embedding(query)\n",
    "    return category\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your query: \")\n",
    "category = classify_query(query)\n",
    "print(f\"Query category: {category}\")\n",
    "\n",
    "# Further actions based on the classification\n",
    "if category == \"personal\":\n",
    "    print(\"Handling personal query...\")\n",
    "    # Handle personal query\n",
    "elif category == \"atomic_habits\":\n",
    "    print(\"Handling Atomic Habits query...\")\n",
    "    # Handle Atomic Habits query\n",
    "else:\n",
    "    print(\"Unknown category. Please clarify your query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_keywords = [atomic_habits_keywords.rstrip() for atomic_habits_keywords in atomic_habits_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "from typing import List\n",
    "\n",
    "# Apply the patch to the OpenAI client\n",
    "# enables response_model keyword\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class ClassificationResponse(BaseModel):\n",
    "    label: Literal[\"ATOMIC_HABITS\", \"PERSONAL\"] = Field(\n",
    "        ...,\n",
    "        description=\"The predicted class label.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def classify(data: str, keywords: List['str']) -> ClassificationResponse:\n",
    "    \"\"\"Perform single-label classification on the input text.\"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        response_model=ClassificationResponse,\n",
    "        messages=[{'role': 'system',\n",
    "                   'content': f'Use these keywords to determine the appropriate classification if any of them match the data then the classification should be ATOMIC_HABITS{\" \".join(keywords)}'\n",
    "                   },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Classify the following text: {data}\",\n",
    "                    },\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationResponse(label='PERSONAL')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('how can I improve my productivity at work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abilities',\n",
       " 'accountability',\n",
       " 'none',\n",
       " 'Person',\n",
       " 'addiction',\n",
       " 'addiction',\n",
       " 'addiction',\n",
       " 'addiction',\n",
       " 'addiction',\n",
       " 'addiction',\n",
       " 'addiction',\n",
       " 'addiction',\n",
       " 'animal behavior',\n",
       " 'animal behavior',\n",
       " 'animal behavior',\n",
       " 'animal behavior',\n",
       " 'animal behavior',\n",
       " 'animal behavior',\n",
       " 'animal behavior',\n",
       " 'athletes',\n",
       " 'athletes',\n",
       " 'athletes',\n",
       " 'athletes',\n",
       " 'athletes',\n",
       " 'athletes',\n",
       " 'atomic habits',\n",
       " 'atomic habits',\n",
       " 'atomic habits',\n",
       " 'automating a habit',\n",
       " 'automating a habit',\n",
       " 'automating a habit',\n",
       " 'automating a habit',\n",
       " 'awareness',\n",
       " 'awareness',\n",
       " 'awareness',\n",
       " 'bad habits',\n",
       " 'bad habits',\n",
       " 'behavior change',\n",
       " 'behavior change',\n",
       " 'behavior change',\n",
       " 'behavior change',\n",
       " 'behavior change',\n",
       " 'behavior change',\n",
       " 'biological considerations',\n",
       " 'biological considerations',\n",
       " 'biological considerations',\n",
       " 'biological considerations',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'the brain',\n",
       " 'breakthrough moments',\n",
       " 'breakthrough moments',\n",
       " 'breakthrough moments',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'four step process',\n",
       " 'choosing the right opportunities',\n",
       " 'choosing the right opportunities',\n",
       " 'choosing the right opportunities',\n",
       " 'choosing the right opportunities',\n",
       " 'choosing the right opportunities',\n",
       " 'choosing the right opportunities',\n",
       " 'compounding effect of small changes',\n",
       " 'compounding effect of small changes',\n",
       " 'compounding effect of small changes',\n",
       " 'compounding effect of small changes',\n",
       " 'compounding effect of small changes',\n",
       " 'compounding effect of small changes',\n",
       " 'compounding effect of small changes',\n",
       " 'compounding effect of small changes',\n",
       " 'cravings',\n",
       " 'cravings',\n",
       " 'cravings',\n",
       " 'cravings',\n",
       " 'cues',\n",
       " 'cues',\n",
       " 'cues',\n",
       " 'cues',\n",
       " 'cues',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'culture',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'environment',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'feedback loops',\n",
       " 'food science',\n",
       " 'food science',\n",
       " 'food science',\n",
       " 'food science',\n",
       " 'food science',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'four,step process of building a habit',\n",
       " 'friction',\n",
       " 'friction',\n",
       " 'friction',\n",
       " 'friction',\n",
       " 'friction',\n",
       " 'friction',\n",
       " 'goals',\n",
       " 'goals',\n",
       " 'goals',\n",
       " 'goals',\n",
       " 'goals',\n",
       " 'the Goldilocks Rule',\n",
       " 'the Goldilocks Rule',\n",
       " 'the Goldilocks Rule',\n",
       " 'good habits',\n",
       " 'good habits',\n",
       " 'good habits',\n",
       " 'good habits',\n",
       " 'good habits',\n",
       " 'good habits',\n",
       " 'good habits',\n",
       " 'good habits',\n",
       " 'habit contract',\n",
       " 'habit contract',\n",
       " 'habit contract',\n",
       " 'habit contract',\n",
       " 'habit contract',\n",
       " 'habit contract',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'habit stacking',\n",
       " 'habit stacking',\n",
       " 'habit stacking',\n",
       " 'habit stacking',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'identity',\n",
       " 'measurements',\n",
       " 'measurements',\n",
       " 'measurements',\n",
       " 'measurements',\n",
       " 'mind,set shifts',\n",
       " 'mind,set shifts',\n",
       " 'mind,set shifts',\n",
       " 'mind,set shifts',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'motivation',\n",
       " 'opportunities, choosing the right',\n",
       " 'opportunities, choosing the right',\n",
       " 'opportunities, choosing the right',\n",
       " 'opportunities, choosing the right',\n",
       " 'outcomes',\n",
       " 'outcomes',\n",
       " 'outcomes',\n",
       " 'outcomes',\n",
       " 'outcomes',\n",
       " 'outcomes',\n",
       " 'outcomes',\n",
       " 'outcomes',\n",
       " 'outcomes',\n",
       " 'pleasure',\n",
       " 'pleasure',\n",
       " 'pleasure',\n",
       " 'pleasure',\n",
       " 'pleasure',\n",
       " 'pleasure',\n",
       " 'pleasure',\n",
       " 'pleasure',\n",
       " 'predictions, making',\n",
       " 'predictions, making',\n",
       " 'predictions, making',\n",
       " 'predictions, making',\n",
       " 'pride',\n",
       " 'pride',\n",
       " 'pride',\n",
       " 'pride',\n",
       " 'pride',\n",
       " 'pride',\n",
       " 'pride',\n",
       " 'pride',\n",
       " 'pride',\n",
       " 'reading resources',\n",
       " 'reading resources',\n",
       " 'reading resources',\n",
       " 'reading resources',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'reflection and review',\n",
       " 'repetition',\n",
       " 'repetition',\n",
       " 'repetition',\n",
       " 'repetition',\n",
       " 'repetition',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'rewards',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'satisfaction',\n",
       " 'self,control',\n",
       " 'self,control',\n",
       " 'self,control',\n",
       " 'self,control',\n",
       " 'self,control',\n",
       " 'the senses',\n",
       " 'the senses',\n",
       " 'the senses',\n",
       " 'the senses',\n",
       " 'the senses',\n",
       " 'the senses',\n",
       " 'the senses',\n",
       " 'the senses',\n",
       " 'social norms',\n",
       " 'social norms',\n",
       " 'social norms',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'imitation of others’ habits',\n",
       " 'success',\n",
       " 'success',\n",
       " 'success',\n",
       " 'success',\n",
       " 'success',\n",
       " 'success',\n",
       " 'success',\n",
       " 'systems',\n",
       " 'systems',\n",
       " 'systems',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'technology',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'tracking a habit',\n",
       " 'vision',\n",
       " 'vision',\n",
       " 'vision',\n",
       " 'weight loss',\n",
       " 'weight loss',\n",
       " 'weight loss']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_path = \"/Users/clydeclarke/Documents/AH_Code_Architecture/data/ah_index.csv\"\n",
    "index_data = pd.read_csv(csv_path)\n",
    "index_data.columns = index_data.columns.str.strip()\n",
    "\n",
    "# Extract concepts and words related to \"Atomic Habits\"\n",
    "atomic_habits_keywords = index_data['concept'].tolist()\n",
    "[atomic_habits_keywords.rstrip() for atomic_habits_keywords in atomic_habits_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can I improve my productivity at work? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: What are the key principles of Atomic Habits? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: Tell me about your family. Classification: label='PERSONAL' Answer: personal\n",
      "Question: How do I build a good habit according to Atomic Habits? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: I'm struggling with staying motivated. Any advice? Classification: label='ATOMIC_HABITS' Answer: personal\n",
      "Question: Can you summarize the book Atomic Habits? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: I want to track my habits more effectively. Classification: label='ATOMIC_HABITS' Answer: personal\n",
      "Question: What is the Two-Minute Rule in Atomic Habits? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: How do I break a bad habit according to James Clear? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: I feel overwhelmed with my daily tasks. Help? Classification: label='PERSONAL' Answer: personal\n",
      "Question: Can you explain the habit loop concept? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: How do you recommend managing time better? Classification: label='ATOMIC_HABITS' Answer: personal\n",
      "Question: What is the significance of identity in habit formation? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: How do I set better goals for myself? Classification: label='ATOMIC_HABITS' Answer: personal\n",
      "Question: What strategies does Atomic Habits suggest for habit stacking? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: I want to develop a better morning routine. Classification: label='ATOMIC_HABITS' Answer: personal\n",
      "Question: How does environment design affect habits? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: I'm interested in learning about habit tracking methods. Classification: label='ATOMIC_HABITS' Answer: personal\n",
      "Question: What are some practical tips from Atomic Habits for maintaining consistency? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n",
      "Question: How do I overcome procrastination? Classification: label='ATOMIC_HABITS' Answer: personal\n",
      "Question: What does James Clear say about the role of rewards in habit formation? Classification: label='ATOMIC_HABITS' Answer: atomic_habits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '/Users/clydeclarke/Documents/AH_router/data/ah_qa.csv'\n",
    "questions = pd.read_csv(file_path)\n",
    "for index, row in questions.iterrows():\n",
    "   print(f'Question: {row['query']} Classification: {classify(row['query'],ah_keywords)} Answer: {row[\"classification\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=CombinedMemory(memories=[ConversationBufferMemory(input_key='input', memory_key='chat_history_lines'), ConversationSummaryMemory(llm=OpenAI(client=<openai.resources.completions.Completions object at 0x114527920>, async_client=<openai.resources.completions.AsyncCompletions object at 0x114518890>, openai_api_key=SecretStr('**********'), openai_proxy=''), input_key='input')]) verbose=True prompt=PromptTemplate(input_variables=['chat_history_lines', 'history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nSummary of conversation:\\n{history}\\nCurrent conversation:\\n{chat_history_lines}\\nHuman: {input}\\nAI:') llm=OpenAI(client=<openai.resources.completions.Completions object at 0x1145b1610>, async_client=<openai.resources.completions.AsyncCompletions object at 0x1145b3dd0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.memory import (\n",
    "    CombinedMemory,\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "conv_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history_lines\", input_key=\"input\"\n",
    ")\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=OpenAI(), input_key=\"input\")\n",
    "# Combined\n",
    "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
    "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Summary of conversation:\n",
    "{history}\n",
    "Current conversation:\n",
    "{chat_history_lines}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\", \"chat_history_lines\"],\n",
    "    template=_DEFAULT_TEMPLATE,\n",
    ")\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT)\n",
    "# conversation.run(\"my name is clyde\")\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building embeddings:   0%|          | 0/98 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/atomic_user/lib/python3.12/multiprocessing/queues.py\", line 264, in _feed\n    obj = _ForkingPickler.dumps(obj)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/atomic_user/lib/python3.12/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\nTypeError: cannot pickle '_thread.RLock' object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     doc_text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Build the index from the document text\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Retrieve the context for a given query\u001b[39;00m\n\u001b[1;32m     28\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the meaning of life?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/AH_router/raptor/FaissRetriever.py:121\u001b[0m, in \u001b[0;36mFaissRetriever.build_from_text\u001b[0;34m(self, doc_text)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(futures, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(futures), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/atomic_user/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/atomic_user/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/atomic_user/lib/python3.12/multiprocessing/queues.py:264\u001b[0m, in \u001b[0;36mQueue._feed\u001b[0;34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# serialize the data before acquiring the lock\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wacquire \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     send_bytes(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/atomic_user/lib/python3.12/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "from raptor.FaissRetriever import FaissRetriever, FaissRetrieverConfig\n",
    "from raptor.EmbeddingModels import OpenAIEmbeddingModel\n",
    "import tiktoken\n",
    "\n",
    "# Create the configuration\n",
    "config = FaissRetrieverConfig(\n",
    "    max_tokens=100,\n",
    "    max_context_tokens=3500,\n",
    "    use_top_k=False,\n",
    "    embedding_model=OpenAIEmbeddingModel(),\n",
    "    question_embedding_model=OpenAIEmbeddingModel(),\n",
    "    top_k=5,\n",
    "    tokenizer=tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    embedding_model_string=\"OpenAI\"\n",
    ")\n",
    "\n",
    "# Create the retriever instance\n",
    "retriever = FaissRetriever(config)\n",
    "\n",
    "# Load the document text from a file or any other source\n",
    "with open(\"/Users/clydeclarke/Documents/AH_router/data/dune.txt\", \"r\") as f:\n",
    "    doc_text = f.read()\n",
    "\n",
    "# Build the index from the document text\n",
    "retriever.build_from_text(doc_text)\n",
    "\n",
    "# Retrieve the context for a given query\n",
    "query = \"What is the meaning of life?\"\n",
    "context = retriever.retrieve(query)\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<raptor.FaissRetriever.FaissRetriever at 0x17351a330>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building embeddings:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/atomic_user/lib/python3.12/multiprocessing/queues.py\", line 264, in _feed\n    obj = _ForkingPickler.dumps(obj)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/atomic_user/lib/python3.12/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\nTypeError: cannot pickle '_thread.RLock' object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AH_router/raptor/FaissRetriever.py:121\u001b[0m, in \u001b[0;36mFaissRetriever.build_from_text\u001b[0;34m(self, doc_text)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(futures, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(futures), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/atomic_user/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/atomic_user/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/atomic_user/lib/python3.12/multiprocessing/queues.py:264\u001b[0m, in \u001b[0;36mQueue._feed\u001b[0;34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# serialize the data before acquiring the lock\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wacquire \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     send_bytes(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/atomic_user/lib/python3.12/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "retriever.build_from_text(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.Stream object at 0x11b33b260>\n"
     ]
    }
   ],
   "source": [
    "import pinecone_rag\n",
    "completion = pinecone_rag.client_openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Replace with the appropriate OpenAI model\n",
    "    messages=[{\"role\":\"system\",\"content\":'You are a useful assistant'},\n",
    "              {\"role\":\"user\", \"content\":'What is your name'},],\n",
    "    max_tokens=300,\n",
    "    stream=True,\n",
    "    presence_penalty=0,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "i=0\n",
    "for chunks in completion:\n",
    "    i = i + 1\n",
    "    mini_chunk = chunks.choices[0].delta.content\n",
    "    print(mini_chunk)\n",
    "    if mini_chunk is None:\n",
    "        mini_chunk = \"\"\n",
    "    text += mini_chunk\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.choices[0].delta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def main() -> None:\n",
    "    stream = await pinecone_rag.client_openai.chat.completions.create(\n",
    "        model=\"your_deployment_name\",\n",
    "        messages = [ {\"role\": \"user\", \"content\": \"What is chatgpt?\"} ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    #async for data in stream:\n",
    "    #    print(data.model_dump_json())\n",
    "    #    print(\"test\")\n",
    "\n",
    "    async for choices in stream:\n",
    "        print(choices.model_dump_json(indent=2))    \n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object main at 0x11aefe500>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atomic_user",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
